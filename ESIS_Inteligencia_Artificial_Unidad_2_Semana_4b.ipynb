{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kazuky1141/Colab_IA/blob/master/ESIS_Inteligencia_Artificial_Unidad_2_Semana_4b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyRFI0IJCsuu"
      },
      "source": [
        "# 4b. Despliegue de su modelo\n",
        "Ahora que tenemos un modelo bien entrenado, es hora de utilizarlo. En este ejercicio, expondremos nuevas imágenes a nuestro modelo y detectaremos las letras correctas del alfabeto del lenguaje de signos. ¡Vamos a empezar!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdYYMU1WCsuv"
      },
      "source": [
        "## 4b.1 Objetivos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeXBHUIjCsuv"
      },
      "source": [
        "* Cargar desde disco un modelo ya entrenado\n",
        "* Reformatear imágenes para un modelo entrenado con imágenes de formato diferente.\n",
        "* Realizar inferencias con imágenes nuevas, nunca vistas por el modelo entrenado y evaluar su rendimiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idwYaoghaqIp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.io as tv_io\n",
        "import torchvision.transforms.v2 as transforms\n",
        "import torchvision.transforms.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo-pr5-zCsuv"
      },
      "source": [
        "## 4b.2 Cargando el Modelo\n",
        "\n",
        "Ahora que estamos en un nuevo cuaderno, vamos a cargar el modelo guardado que hemos entrenado. Nuestro guardado del ejercicio anterior creó una carpeta llamada «asl_model». Podemos cargar el modelo seleccionando la misma carpeta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpLTn1ZUL9-Y"
      },
      "source": [
        "Dado que nuestro modelo utiliza un [módulo personalizado](https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html), necesitaremos cargar el código de esa clase. Hemos guardado una copia del código en [utils.py](./uitls.py)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y p7zip-full\n",
        "!wget https://github.com/ichaparroc/IA-EPIS/raw/refs/heads/main/ASL.7z -O ASL.7Z\n",
        "!7z x ASL.7Z\n",
        "!rm -f ASL.7Z\n",
        "!rm -f sign_mnist_train.csv\n",
        "!rm -f sign_mnist_valid.csv"
      ],
      "metadata": {
        "id": "RYqCsmeOM3rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiI87fNVZdRl"
      },
      "outputs": [],
      "source": [
        "from utils import MyConvBlock"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYbZ4ZRbL9-a"
      },
      "source": [
        "Ahora que tenemos una definición para `MyConvBlock`, podemos usar [torch.load](https://pytorch.org/docs/stable/generated/torch.load.html) para cargar un modelo desde una ruta. Podemos usar `map_location` para especificar el dispositivo. Cuando imprimimos el modelo, ¿se ve igual que en el último cuaderno?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Xy1JuL-Csuv"
      },
      "outputs": [],
      "source": [
        "model = torch.load('model.pth', map_location=device, weights_only=False)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIjqbFCFCsuv"
      },
      "source": [
        "También podemos verificar si el modelo está en nuestra GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ulCst9GCsuw"
      },
      "outputs": [],
      "source": [
        "next(model.parameters()).device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBBneFIXCsuw"
      },
      "source": [
        "## 4b.3 Preparar una imagen para el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gffCLeacCsuw"
      },
      "source": [
        "Ahora es el momento de utilizar el modelo para hacer predicciones sobre nuevas imágenes que nunca ha visto antes. Esto también se llama inferencia. Tenemos un conjunto de imágenes en la carpeta `data/asl_images`. Intenta abrirla utilizando el navegador de la izquierda y explora las imágenes.\n",
        "\n",
        "Te darás cuenta de que las imágenes que tenemos son de mucha mayor resolución que las imágenes de nuestro conjunto de datos. También son en color. Recuerda que nuestras imágenes en el conjunto de datos eran de 28x28 píxeles y en escala de grises. Es importante tener en cuenta que siempre que hacemos predicciones con un modelo, la entrada debe coincidir con la forma de los datos con los que se entrenó el modelo. Para este modelo, el conjunto de datos de entrenamiento tenía la forma: (27455, 28, 28, 1). Esto correspondía a 27455 imágenes de 28 por 28 píxeles cada una con un canal de color (escala de grises)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXeujDRFCsuw"
      },
      "source": [
        "### 4b.3.1 Mostrar las imágenes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gLvKQ1yCsuw"
      },
      "source": [
        "Cuando utilicemos nuestro modelo para hacer predicciones sobre nuevas imágenes, será útil mostrar también la imagen. Para ello, podemos utilizar la biblioteca matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzzai3QZCsuw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "def show_image(image_path):\n",
        "    image = mpimg.imread(image_path)\n",
        "    plt.imshow(image, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFzwvkalCsuw"
      },
      "outputs": [],
      "source": [
        "show_image('b.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORPUII6qCsux"
      },
      "source": [
        "### 4b.3.2 Escalado de las imágenes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYcawjxlCsux"
      },
      "source": [
        "Las imágenes de nuestro conjunto de datos eran de 28x28 píxeles y en escala de grises. Necesitamos asegurarnos de pasar el mismo tamaño e imágenes en escala de grises a nuestro método para la predicción. Hay algunas formas de editar imágenes con Python, pero TorchVision también tiene la función [read_image](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html). Podemos hacerle saber qué tipo de imagen leer con [ImageReadMode](https://pytorch.org/vision/stable/generated/torchvision.io.ImageReadMode.html#torchvision.io.ImageReadMode)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDJu6JrEffRP"
      },
      "outputs": [],
      "source": [
        "image = tv_io.read_image('b.png', tv_io.ImageReadMode.GRAY)\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY6jdx40L9-f"
      },
      "source": [
        "Fijémonos en la forma de la imagen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iJ4A8YagRvP"
      },
      "outputs": [],
      "source": [
        "image.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTbsFGCOL9-g"
      },
      "source": [
        "Esta imagen es mucho más grande que con la que entrenamos. Podemos utilizar [TorchVision](https://pytorch.org/vision/stable/index.html)'s [Transforms](https://pytorch.org/vision/0.9/transforms.html) de nuevo para obtener los datos en la forma que nuestro modelo espera.\n",
        "\n",
        "Lo haremos:\n",
        "* Convertiremos la imagen a float con [ToDtype](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.ToDtype.html)\n",
        "  * Estableceremos `scale` a `True` para convertir de [0, 255] a [0, 1].\n",
        "* [Redimensionar](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.Resize.html#torchvision.transforms.v2.Resize) la imagen para que sea de 28 x 28 píxeles\n",
        "* Convertir las imágenes a [Escala de grises](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.Grayscale.html#torchvision.transforms.v2.Grayscale)\n",
        "  * Este paso no hace nada ya que nuestros modelos ya están en escala de grises, pero lo hemos añadido aquí para mostrar una forma alternativa de obtener imágenes en escala de grises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNp6Q9cBe8Xw"
      },
      "outputs": [],
      "source": [
        "IMG_WIDTH = 28\n",
        "IMG_HEIGHT = 28\n",
        "\n",
        "preprocess_trans = transforms.Compose([\n",
        "    transforms.ToDtype(torch.float32, scale=True), # Converts [0, 255] to [0, 1]\n",
        "    transforms.Resize((IMG_WIDTH, IMG_HEIGHT)),\n",
        "    transforms.Grayscale()  # From Color to Gray\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy8QgFYaL9-i"
      },
      "source": [
        "Probemos `preprocess_trans` en una imagen para asegurarnos de que funciona correctamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBS7SjnBhIe6"
      },
      "outputs": [],
      "source": [
        "processed_image = preprocess_trans(image)\n",
        "processed_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrtrwwRuL9-i"
      },
      "source": [
        "Los números parecen correctos, pero ¿y la forma?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dmpv4NDnL9-j"
      },
      "outputs": [],
      "source": [
        "processed_image.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBQwvr_oL9-j"
      },
      "source": [
        "A continuación, vamos a trazar la imagen para ver si se parece a lo que hemos entrenado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbVyXC5uL9-j"
      },
      "outputs": [],
      "source": [
        "plot_image = F.to_pil_image(processed_image)\n",
        "plt.imshow(plot_image, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaptN5XrL9-k"
      },
      "source": [
        "¡Se ve bien! Pasémoslo a nuestro modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygDDddrVL9-m"
      },
      "source": [
        "### 4b.4 Hacer predicciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYl2JcUJCsuy"
      },
      "source": [
        "Bien, ¡ahora estamos listos para predecir! Nuestro modelo sigue esperando un lote de imágenes. Si [squeeze](https://pytorch.org/docs/stable/generated/torch.squeeze.htmlhttps://pytorch.org/docs/stable/generated/torch.squeeze.html) elimina dimensiones de 1, [unsqueeze](https://pytorch.org/docs/stable/generated/torch.unsqueeze.htmlhttps://pytorch.org/docs/stable/generated/torch.unsqueeze.html) añade una dimensión de 1 en el índice que especifiquemos. La primera dimensión suele ser la dimensión del lote, por lo que podemos decir `.unsqueeze(0)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-xZGe1SCsux"
      },
      "outputs": [],
      "source": [
        "batched_image = processed_image.unsqueeze(0)\n",
        "batched_image.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McLAA39yL9-n"
      },
      "source": [
        "A continuación, debemos asegurarnos de que el tensor de entrada está en el mismo `dispositivo` que el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J399TjATL9-n"
      },
      "outputs": [],
      "source": [
        "batched_image_gpu = batched_image.to(device)\n",
        "batched_image_gpu.device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AgKK_X0L9-o"
      },
      "source": [
        "Ahora estamos listos para pasarlo al modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhLIaQIhkE0c"
      },
      "outputs": [],
      "source": [
        "output = model(batched_image_gpu)\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCleBaVCCsuz"
      },
      "source": [
        "### 4b.4.1 Comprender la predicción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNIv09wxCsuz"
      },
      "source": [
        "Las predicciones tienen el formato de una matriz de 24 longitudes. Cuanto mayor sea el valor, más probable es que la imagen de entrada pertenezca a la clase correspondiente. Hagámoslo un poco más legible. Podemos empezar por encontrar qué elemento de la matriz representa la mayor probabilidad. Esto puede hacerse fácilmente con la biblioteca numpy y la función [argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm6J4jxcCsuz"
      },
      "outputs": [],
      "source": [
        "prediction = output.argmax(dim=1).item()\n",
        "prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpZzxObRCsuz"
      },
      "source": [
        "Cada elemento de la matriz de predicción representa una letra posible del alfabeto de la lengua de signos. Recuerda que j y z no son opciones porque implican mover la mano, y sólo estamos tratando con fotos fijas. Creemos una correspondencia entre el índice de la matriz de predicciones y la letra correspondiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ExBb6fMCsuz"
      },
      "outputs": [],
      "source": [
        "# Alphabet does not contain j or z because they require movement\n",
        "alphabet = \"abcdefghiklmnopqrstuvwxy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXy2Ze9kCsuz"
      },
      "source": [
        "Ahora podemos pasar nuestro índice de predicción para encontrar la letra correspondiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNxyHNZmCsuz"
      },
      "outputs": [],
      "source": [
        "alphabet[prediction]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs2_6guPCsuz"
      },
      "source": [
        "#### Ejercicio: Póngalo todo junto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KobsgLirCsuz"
      },
      "source": [
        "Pongamos todo en una función para que podamos hacer predicciones sólo a partir del archivo de imagen. Impleméntalo en la función de abajo usando las funciones y pasos anteriores. Si necesitas ayuda, puedes revelar la solución haciendo clic en los tres puntos de abajo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3prZEMgCsuz"
      },
      "outputs": [],
      "source": [
        "def predict_letter(file_path):\n",
        "    # Show image\n",
        "    FIXME\n",
        "    # Load and grayscale image\n",
        "    image = FIXME\n",
        "    # Transform image\n",
        "    image = FIXME\n",
        "    # Batch image\n",
        "    image = FIXME\n",
        "    # Send image to correct device\n",
        "    image = FIXME\n",
        "    # Make prediction\n",
        "    output = FIXME\n",
        "    # Find max index\n",
        "    prediction = FIXME\n",
        "    # Convert prediction to letter\n",
        "    predicted_letter = FIXME\n",
        "    # Return prediction\n",
        "    return predicted_letter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWqiMAhwCsu_"
      },
      "outputs": [],
      "source": [
        "predict_letter(\"b.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNZSUoinCsvA"
      },
      "source": [
        "Utilicemos también la función con la letra 'a' en el datset asl_images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk0F5w53CsvA"
      },
      "outputs": [],
      "source": [
        "predict_letter(\"a.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFaxufFbCsvA"
      },
      "source": [
        "## 4b.5 Resumen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtsAm4TJCsvA"
      },
      "source": [
        "Buen trabajo con estos ejercicios. Has pasado por todo el proceso de entrenamiento de un modelo de alta precisión desde cero, y luego has utilizado el modelo para hacer nuevas y valiosas predicciones. Si tienes algo de tiempo, te animamos a que tomes fotos con tu webcam, las subas soltándolas en la carpeta data/asl_images, y pruebes el modelo con ellas. Para Mac puedes utilizar Photo Booth. Para Windows puedes seleccionar la aplicación Cámara en la pantalla de inicio. Esperamos que lo pruebes. Es una buena oportunidad para aprender algo de lenguaje de signos. Por ejemplo, prueba con las letras de tu nombre.\n",
        "\n",
        "Podemos imaginar cómo se podría utilizar este modelo en una aplicación para enseñar a alguien el lenguaje de signos, o incluso ayudar a alguien que no puede hablar a interactuar con un ordenador."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preguntas\n",
        "\n",
        "1. ¿Por qué es importante que las imágenes utilizadas para hacer predicciones tengan el mismo formato (dimensiones y color) que las imágenes con las que se entrenó el modelo?\n",
        "\n",
        "2. Describe los pasos necesarios para procesar una imagen antes de hacer una predicción con el modelo entrenado. ¿Qué transformaciones se deben aplicar?\n",
        "\n",
        "3. ¿Qué es la \"dimensión del lote\" (batch) y por qué es necesario agregarla cuando hacemos predicciones con el modelo?\n",
        "\n",
        "4. ¿Cómo interpreta el modelo las predicciones que genera? Explica el formato de salida y cómo se convierte en una letra del alfabeto de lengua de signos."
      ],
      "metadata": {
        "id": "5Fji1dpZSzkw"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}